{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, on mac m1, use conda to install transformers:\n",
    "```\n",
    "conda install -c huggingface transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "APPLY_SAMPLE = False\n",
    "SAMPLE_SIZE = 500\n",
    "\n",
    "TWEET_DATA = '../data/01_raw/product_sentiment.csv'\n",
    "LABEL_MAPPING = { \n",
    "    \"No emotion toward brand or product\": {\n",
    "        \"alt_label\": \"NEU\",\n",
    "        \"class\": 1,\n",
    "    },\n",
    "    \"Positive emotion\": {\n",
    "        \"alt_label\": \"POS\",\n",
    "        \"class\": 2,\n",
    "    },\n",
    "    \"Negative emotion\": {\n",
    "        \"alt_label\": \"NEG\",\n",
    "        \"class\": 0,\n",
    "    },\n",
    "}\n",
    "USE_CLEANED_TWEET = True\n",
    "\n",
    "BASE_MODEL = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "# BASE_MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "# save filepath\n",
    "processed_indicator = 'raw'\n",
    "if USE_CLEANED_TWEET:\n",
    "    processed_indicator = 'cleaned'\n",
    "PREDICTION_FILEPATH = f\"../data/07_model_output/{BASE_MODEL}/{processed_indicator}_tweet_predicitons.csv\"\n",
    "\n",
    "if APPLY_SAMPLE:\n",
    "    PREDICTION_FILEPATH = PREDICTION_FILEPATH.replace(\".csv\", f\"_sample_{SAMPLE_SIZE}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TWEET_DATA)\n",
    "df.rename(columns={'is_there_an_emotion_directed_at_a_brand_or_product': 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "1. Remove \"I can't tell\" labels - 156 rows (1.7% of data) and any missing tweets (1 data point)\n",
    "2. Clean tweet text - remove links and `@` / `#` prefixes\n",
    "3. Rename the labels - see `LABEL_MAPPING`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def _strip_links(text):\n",
    "    link_regex = re.compile(\"((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)\", re.DOTALL)\n",
    "    links = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], \", \")\n",
    "    return text\n",
    "\n",
    "\n",
    "def _strip_all_entities(text):\n",
    "    entity_prefixes = [\"@\", \".@\", \"#\", \".#\"]\n",
    "    # replace all other punctuation with a space\n",
    "    # for separator in string.punctuation:\n",
    "    #     if separator not in entity_prefixes:\n",
    "    #         text = text.replace(separator, \" \")\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def preprocess_tweet_text(text):\n",
    "    return _strip_all_entities(_strip_links(text))\n",
    "\n",
    "def rename_labels(row):\n",
    "    row['alt_label'] = LABEL_MAPPING[row['label']]['alt_label']\n",
    "    row['class'] = LABEL_MAPPING[row['label']]['class']\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len before cln: 9093, len after cln: 8936\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_text_cln</th>\n",
       "      <th>alt_label</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Know about ? Awesome iPad/iPhone app that you'...</td>\n",
       "      <td>POS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Can not wait for 2 also. They should sale them...</td>\n",
       "      <td>POS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                         tweet_text  \\\n",
       "0      0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1      1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2      2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at             label  \\\n",
       "0                          iPhone  Negative emotion   \n",
       "1              iPad or iPhone App  Positive emotion   \n",
       "2                            iPad  Positive emotion   \n",
       "\n",
       "                                      tweet_text_cln alt_label  class  \n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...       NEG      0  \n",
       "1  Know about ? Awesome iPad/iPhone app that you'...       POS      2  \n",
       "2  Can not wait for 2 also. They should sale them...       POS      2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove unknown labels and drop na\n",
    "df_cln = df.copy()[df['label'] != \"I can't tell\"].dropna(subset=['tweet_text'])\n",
    "len_before, len_after = len(df), len(df_cln)\n",
    "print(f'len before cln: {len_before}, len after cln: {len_after}')\n",
    "\n",
    "# clean text\n",
    "df_cln['tweet_text_cln'] = df_cln['tweet_text'].apply(preprocess_tweet_text)\n",
    "\n",
    "# rename labels\n",
    "df_cln = df_cln.apply(rename_labels, axis=1)\n",
    "df_cln.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model\n",
    "\n",
    "Using [pre-trained sentiment model](https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis?text=net-purpose+is+cool), fine-tuned on ~40k tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100701cb257146b5bcef17f6f00eecbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sentence: \"lots of love for this\", predicted sentiment: POS(0.99)\n",
      "test sentence: \"windows os is bad\", predicted sentiment: NEG(0.98)\n",
      "test sentence: \"london has a high population\", predicted sentiment: NEU(0.84)\n",
      "test sentence: \"london is cool\", predicted sentiment: POS(0.99)\n"
     ]
    }
   ],
   "source": [
    "# use sentiment model fine-tuned on tweet data\n",
    "sentiment_pipeline = pipeline(model=BASE_MODEL)\n",
    "\n",
    "# test\n",
    "data = [\"lots of love for this\", \"windows os is bad\", \"london has a high population\", \"london is cool\"]\n",
    "predictions = sentiment_pipeline(data)\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"test sentence: \\\"{data[i]}\\\", predicted sentiment: {pred['label']}({pred['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = 'tweet_text'\n",
    "if USE_CLEANED_TWEET:\n",
    "    text_column = 'tweet_text_cln'\n",
    "\n",
    "# sample data if required\n",
    "df_pred = df_cln.copy()\n",
    "if APPLY_SAMPLE:\n",
    "    df_pred = df_pred.sample(SAMPLE_SIZE)\n",
    "\n",
    "# tweet list\n",
    "tweets = df_pred[text_column].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 71/8936 [00:20<43:05,  3.43it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tommy/dev/personal/net-purpose/ml-take-home-test/exploration/02b_pretrained_sentiment.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tommy/dev/personal/net-purpose/ml-take-home-test/exploration/02b_pretrained_sentiment.ipynb#ch0000012?line=4'>5</a>\u001b[0m pred \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tommy/dev/personal/net-purpose/ml-take-home-test/exploration/02b_pretrained_sentiment.ipynb#ch0000012?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m tweet \u001b[39min\u001b[39;00m tqdm(tweets):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tommy/dev/personal/net-purpose/ml-take-home-test/exploration/02b_pretrained_sentiment.ipynb#ch0000012?line=6'>7</a>\u001b[0m     pred\u001b[39m.\u001b[39mextend(sentiment_pipeline(tweet))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:125\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py?line=91'>92</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py?line=92'>93</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py?line=93'>94</a>\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py?line=94'>95</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py?line=122'>123</a>\u001b[0m \u001b[39m        If `self.return_all_scores=True`, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py?line=123'>124</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py?line=124'>125</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py?line=125'>126</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py?line=126'>127</a>\u001b[0m         \u001b[39m# This pipeline is odd, and return a list when single item is run\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py?line=127'>128</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m [result]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py:1026\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py?line=1023'>1024</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py?line=1024'>1025</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py?line=1025'>1026</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py:1033\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py?line=1030'>1031</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py?line=1031'>1032</a>\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py?line=1032'>1033</a>\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py?line=1033'>1034</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py?line=1034'>1035</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py:943\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py?line=940'>941</a>\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py?line=941'>942</a>\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py?line=942'>943</a>\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py?line=943'>944</a>\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/base.py?line=944'>945</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:137\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py?line=135'>136</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward\u001b[39m(\u001b[39mself\u001b[39m, model_inputs):\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/pipelines/text_classification.py?line=136'>137</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1205\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1196'>1197</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1197'>1198</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1198'>1199</a>\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1199'>1200</a>\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1200'>1201</a>\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1201'>1202</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1202'>1203</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1204'>1205</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1205'>1206</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1206'>1207</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1207'>1208</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1208'>1209</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1209'>1210</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1210'>1211</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1211'>1212</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1212'>1213</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1213'>1214</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1214'>1215</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1215'>1216</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=1216'>1217</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:847\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=837'>838</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=839'>840</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=840'>841</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=841'>842</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=844'>845</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=845'>846</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=846'>847</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=847'>848</a>\u001b[0m     embedding_output,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=848'>849</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=849'>850</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=850'>851</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=851'>852</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=852'>853</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=853'>854</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=854'>855</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=855'>856</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=856'>857</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=857'>858</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=858'>859</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=859'>860</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:523\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=513'>514</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=514'>515</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=515'>516</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=519'>520</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=520'>521</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=521'>522</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=522'>523</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=523'>524</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=524'>525</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=525'>526</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=526'>527</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=527'>528</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=528'>529</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=529'>530</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=530'>531</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=532'>533</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=533'>534</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:409\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=396'>397</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=397'>398</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=398'>399</a>\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=405'>406</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=406'>407</a>\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=407'>408</a>\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=408'>409</a>\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=409'>410</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=410'>411</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=411'>412</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=412'>413</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=413'>414</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=414'>415</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=415'>416</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=417'>418</a>\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:336\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=325'>326</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=326'>327</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=327'>328</a>\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=333'>334</a>\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=334'>335</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=335'>336</a>\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=336'>337</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=337'>338</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=338'>339</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=339'>340</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=340'>341</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=341'>342</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=342'>343</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=343'>344</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=344'>345</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=345'>346</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:200\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=189'>190</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=190'>191</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=191'>192</a>\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=197'>198</a>\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=198'>199</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=199'>200</a>\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=201'>202</a>\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=202'>203</a>\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=203'>204</a>\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py?line=204'>205</a>\u001b[0m     is_cross_attention \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# full data takes ~35mins on cpu\n",
    "# for loop is just as fast as `sentiment_pipeline(tweets)` on cpu\n",
    "\n",
    "# TODO detect device (cuda) and change prediction function accordingly - for loop will likely be slower (?)\n",
    "pred = []\n",
    "for tweet in tqdm(tweets):\n",
    "    pred.extend(sentiment_pipeline(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (71) does not match length of index (8936)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/tommy/dev/personal/net-purpose/ml-take-home-test/exploration/02b_pretrained_sentiment.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tommy/dev/personal/net-purpose/ml-take-home-test/exploration/02b_pretrained_sentiment.ipynb#ch0000013?line=0'>1</a>\u001b[0m \u001b[39m# add to df\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tommy/dev/personal/net-purpose/ml-take-home-test/exploration/02b_pretrained_sentiment.ipynb#ch0000013?line=1'>2</a>\u001b[0m df_pred[\u001b[39m'\u001b[39m\u001b[39mpred_label\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [prediction[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m prediction \u001b[39min\u001b[39;00m pred]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tommy/dev/personal/net-purpose/ml-take-home-test/exploration/02b_pretrained_sentiment.ipynb#ch0000013?line=2'>3</a>\u001b[0m df_pred[\u001b[39m'\u001b[39m\u001b[39mpred_scores\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [prediction[\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m prediction \u001b[39min\u001b[39;00m pred]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tommy/dev/personal/net-purpose/ml-take-home-test/exploration/02b_pretrained_sentiment.ipynb#ch0000013?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrename_pred_label\u001b[39m(x):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3651'>3652</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3652'>3653</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3653'>3654</a>\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3654'>3655</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3821'>3822</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3822'>3823</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3823'>3824</a>\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3824'>3825</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3829'>3830</a>\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3830'>3831</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3831'>3832</a>\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3833'>3834</a>\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3834'>3835</a>\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3835'>3836</a>\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3836'>3837</a>\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3837'>3838</a>\u001b[0m     ):\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3838'>3839</a>\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=3839'>3840</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py:4535\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=4531'>4532</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=4533'>4534</a>\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=4534'>4535</a>\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/frame.py?line=4535'>4536</a>\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/common.py:557\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/common.py?line=552'>553</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/common.py?line=553'>554</a>\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/common.py?line=554'>555</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/common.py?line=555'>556</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/common.py?line=556'>557</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/common.py?line=557'>558</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/common.py?line=558'>559</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/common.py?line=559'>560</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/common.py?line=560'>561</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/net-purpose-2/lib/python3.9/site-packages/pandas/core/common.py?line=561'>562</a>\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (71) does not match length of index (8936)"
     ]
    }
   ],
   "source": [
    "# add to df\n",
    "df_pred['pred_label'] = [prediction['label'] for prediction in pred]\n",
    "df_pred['pred_scores'] = [prediction['score'] for prediction in pred]\n",
    "\n",
    "def rename_pred_label(x):\n",
    "    labels = {'LABEL_0':\"NEG\", 'LABEL_1': \"NEU\", 'LABEL_2': \"POS\"}\n",
    "    if x in labels.keys():\n",
    "        x = labels[x]\n",
    "    return x\n",
    "\n",
    "df_pred['pred_label'] = df_pred['pred_label'].apply(rename_pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "df_pred.to_csv(PREDICTION_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/07_model_output/finiteautomata/bertweet-base-sentiment-analysis/cleaned_tweet_predicitons.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREDICTION_FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6324977618621307"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_pred = pd.read_csv(PREDICTION_FILEPATH)\n",
    "\n",
    "accuracy_score(df_pred['alt_label'], df_pred['pred_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(df_pred['alt_label'], df_pred['pred_label'], output_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NEG': {'precision': 0.33712512926577043,\n",
       "  'recall': 0.5719298245614035,\n",
       "  'f1-score': 0.42420299284320107,\n",
       "  'support': 570},\n",
       " 'NEU': {'precision': 0.7623318385650224,\n",
       "  'recall': 0.6310319227913883,\n",
       "  'f1-score': 0.6904955320877335,\n",
       "  'support': 5388},\n",
       " 'POS': {'precision': 0.548874323168994,\n",
       "  'recall': 0.6467427803895232,\n",
       "  'f1-score': 0.5938029905965777,\n",
       "  'support': 2978},\n",
       " 'accuracy': 0.6324977618621307,\n",
       " 'macro avg': {'precision': 0.5494437636665955,\n",
       "  'recall': 0.616568175914105,\n",
       "  'f1-score': 0.5695005051758374,\n",
       "  'support': 8936},\n",
       " 'weighted avg': {'precision': 0.6640726280513758,\n",
       "  'recall': 0.6324977618621307,\n",
       "  'f1-score': 0.6412859152647652,\n",
       "  'support': 8936}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "addbd1d925946546e7920bcd25d9d758f9f924699d9f1482dd9e7588f0b49d8e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('net-purpose')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
